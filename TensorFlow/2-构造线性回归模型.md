## 构造线性回归模型


#### 生成测试数据

```
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

# 随机生成1000个点，围绕在y=0.1x+0.3直线周围
num_points = 1000
vectors_set = []
for i in range(num_points):
    x1 = np.random.normal(0.0, 0.55)
    y1 = x1 * 0.1 + 0.3 + np.random.normal(0.0, 0.03)
    vectors_set.append([x1, y1])

# 取出横纵坐标集合
x_data = [v[0] for v in vectors_set]
y_data = [v[1] for v in vectors_set]

# 展示1000个点的坐标系视图
plt.scatter(x_data,y_data,c='r')
plt.show()
```

## 利用梯度下降法拟合数据

```
# 生成1维的W矩阵变量，取值[-1, 1]间随机数
W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name='W')
# 生成1维的b矩阵变量，初始值0
b = tf.Variable(tf.zeros([1]), name='b')
# 经过计算的出预估值y
y = W * x_data + b

# 以4预估值y和实际值y_data之间的均方误差作为损失
loss =tf.reduce_mean(tf.square(y - y_data), name='loss')
# 初始化梯度下降法优化器，用于优化参数
optimizer = tf.train.GradientDescentOptimizer(0.5)
# 训练过程就是使用优化器，最小化误差值
train = optimizer.minimize(loss, name='train')

# 初始化变量并打印
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print ('W=', W.eval(), "b=", b.eval(), 'loss=', loss.eval())

    # 执行20次训练
    for step in range(20):
        sess.run(train)
        print ('W=', W.eval(), "b=", b.eval(), 'loss=', loss.eval())
    # 在图中展示
    plt.scatter(x_data,y_data,c='r')
    plt.plot(x_data, sess.run(W)*x_data+sess.run(b))
    plt.show()
```
